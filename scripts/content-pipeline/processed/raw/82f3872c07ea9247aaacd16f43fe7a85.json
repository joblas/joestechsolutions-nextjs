{
  "id": "82f3872c07ea9247aaacd16f43fe7a85",
  "source_type": "article",
  "source_url": "https://blog.google/innovation-and-ai/technology/developers-tools/kaggle-community-benchmarks/",
  "title": "Introducing Community Benchmarks on Kaggle",
  "author": "Michael Aaron; Meg Risdal",
  "publish_date": "2026-01-14T00:00:00",
  "raw_content": "---\ntitle: Introducing Community Benchmarks on Kaggle\nauthor: Michael Aaron; Meg Risdal\nurl: https://blog.google/innovation-and-ai/technology/developers-tools/kaggle-community-benchmarks/\nhostname: blog.google\ndescription: Community Benchmarks on Kaggle lets the community build, share and run custom evaluations for AI models.\nsitename: Google\ndate: 2026-01-14\ntags: ['None', 'Developer tools,AI']\n---\nIntroducing Community Benchmarks on Kaggle\nToday, Kaggle is launching Community Benchmarks, which lets the global AI community design, run and share their own custom benchmarks for evaluating AI models. This is the next step after we launched Kaggle Benchmarks last year, to provide trustworthy and transparent access to evaluations from top-tier research groups like Meta’s MultiLoKo and Google’s FACTS suite.\nWhy community-driven evaluation matters\nAI capabilities have evolved so rapidly that it’s become difficult to evaluate model performance. Not long ago, a single accuracy score on a static dataset was enough to determine model quality. But today, as LLMs evolve into reasoning agents that collaborate, write code and use tools, those static metrics and simple evaluations are no longer sufficient.\nKaggle Community Benchmarks provide developers with a transparent way to validate their specific use cases and bridge the gap between experimental code and production-ready applications.\nThese real-world use cases demand a more flexible and transparent evaluation framework. Kaggle’s Community Benchmarks provide a more dynamic, rigorous and continuously evolving approach to AI model evaluation — one shaped by the users building and deploying these systems everyday.\nHow to build your own benchmarks on Kaggle\nBenchmarks start with building tasks, which can range from evaluating multi-step reasoning and code generation to testing tool use or image recognition. Once you have tasks, you can add them to a benchmark to evaluate and rank selected models by how they perform across the tasks in the benchmark.\nHere’s how you can get started:\n- Create a task: Tasks test an AI model’s performance on a specific problem. They allow you to run reproducible tests across different models to compare their accuracy and capabilities.\n- Create a benchmark: Once you have created one or more tasks, you can group them into a Benchmark. A benchmark allows you to run tasks across a suite of leading AI models and generate a leaderboard to track and compare their performance.\nOnce you build your benchmark, here’s what benefits you’ll see:\n- Broad model access: Free access (within quota limits) to state-of-the-art models from labs like Google, Anthropic, DeepSeek and more.\n- Reproducibility: Benchmarks capture exact outputs and model interactions so results can be audited and verified.\n- Complex interactions: They support testing for multi-modal inputs, code execution, tool use and multi-turn conversations.\n- Rapid prototyping: They allow you to quickly design and iterate on creative new tasks.\nThese powerful capabilities are powered by the new kaggle-benchmarks SDK. Here are a few resources for getting started:\n- Benchmarks Cookbook: A guide to advanced features and use cases.\n- Example tasks: Get inspired with a variety of pre-built tasks.\n- Getting started: How to create your first task & benchmark\nHow we’re shaping the future of AI evaluation\nThe future of AI progress depends on how models are evaluated. With Kaggle Community Benchmarks, Kagglers are no longer just testing models, they’re helping shape the next generation of intelligence.\nReady to build? Try Community Benchmarks today.",
  "metadata": {
    "site": "Google"
  },
  "status": "ingested",
  "blog_content": null,
  "social_content": null,
  "google_doc_id": null,
  "error_message": null,
  "created_at": "2026-01-24T09:07:35.459946",
  "updated_at": "2026-01-24T09:07:35.459946"
}