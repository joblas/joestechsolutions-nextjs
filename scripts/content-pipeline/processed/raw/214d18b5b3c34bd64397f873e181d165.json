{
  "id": "214d18b5b3c34bd64397f873e181d165",
  "source_type": "article",
  "source_url": "https://techcrunch.com/2026/01/24/a-new-test-for-ai-labs-are-you-even-trying-to-make-money/",
  "title": "A new test for AI labs: Are you even trying to make money? | TechCrunch",
  "author": "Russell Brandom",
  "publish_date": "2026-01-24T00:00:00",
  "raw_content": "---\ntitle: A new test for AI labs: Are you even trying to make money? | TechCrunch\nauthor: Russell Brandom\nurl: https://techcrunch.com/2026/01/24/a-new-test-for-ai-labs-are-you-even-trying-to-make-money/\nhostname: techcrunch.com\ndescription: It’s getting hard to tell which AI labs. are actually trying to make money. We created a rating system to help sort it out.\nsitename: TechCrunch\ndate: 2026-01-24\ncategories: ['AI']\ntags: ['safe superintelligence,world labs,thinking machines lab,foundation models,humans&']\n---\nWe’re in a unique moment for AI companies building their own foundation model.\nFirst, there is a whole generation of industry veterans who made their name at major tech companies and are now going solo. You also have legendary researchers with immense experience but ambiguous commercial aspirations. There’s a clear chance that at least some of these new labs will become OpenAI-sized behemoths, but there’s also room for them to putter around doing interesting research without worrying too much about commercialization.\nThe end result? It’s getting hard to tell who is actually trying to make money.\nTo make things simpler, I’m proposing a kind of sliding scale for any company making a foundation model. It’s a five-level scale where it doesn’t matter if you’re actually making money – only if you’re trying to. The idea here is to measure ambition, not success.\nThink of it in these terms:\n- Level 5: We are already making millions of dollars every day, thank you very much.\n- Level 4: We have a detailed multi-stage plan to become the richest human beings on Earth.\n- Level 3: We have many promising product ideas, which will be revealed in the fullness of time.\n- Level 2: We have the outlines of a concept of a plan.\n- Level 1: True wealth is when you love yourself.\nThe big names are all at Level 5: OpenAI, Anthropic, Gemini, and so on. The scale gets more interesting with the new generation of labs launching now, with big dreams but ambitions that can be harder to read.\nCrucially, the people involved in these labs can generally choose whatever level they want. There’s so much money in AI right now that no one is going to interrogate them for a business plan. Even if the lab is just a research project, investors will count themselves happy to be involved. If you aren’t particularly motivated to become a billionaire, you might well live a happier life at Level 2 than at Level 5.\nDisrupt 2026 Tickets: One-time offer\nTickets are live! Save up to $680 while these rates last, and be among the first 500 registrants to get 50% off your +1 pass. TechCrunch Disrupt brings top leaders from Google Cloud, Netflix, Microsoft, Box, a16z, Hugging Face, and more to 250+ sessions designed to fuel growth and sharpen your edge. Connect with hundreds of innovative startups and join curated networking that drives deals, insights, and inspiration.\nDisrupt 2026 Tickets: One-time offer\nTickets are live! Save up to $680 while these rates last, and be among the first 500 registrants to get 50% off your +1 pass. TechCrunch Disrupt brings top leaders from Google Cloud, Netflix, Microsoft, Box, a16z, Hugging Face, and more to 250+ sessions designed to fuel growth and sharpen your edge. Connect with hundreds of innovative startups and join curated networking that drives deals, insights, and inspiration.\nThe problems arise because it isn’t always clear where an AI lab lands on the scale — and a lot of the AI industry’s current drama comes from that confusion. Much of the anxiety over OpenAI’s conversion from a non-profit came because the lab spent years at Level 1, then jumped to Level 5 almost overnight. On the other side, you might argue that Meta’s early AI research was firmly at Level 2, when what the company really wanted was Level 4.\nWith that in mind, here’s a quick rundown of four of the biggest contemporary AI labs, and how they measure up on the scale.\nHumans&\nHumans& was the big AI news this week, and part of the inspiration for coming up with this whole scale. The founders have a compelling pitch for the next generation of AI models, with scaling laws giving way to an emphasis on communication and coordination tools.\nBut for all the glowing press, Humans& has been coy about how that would translate into actual monetizable products. It seems it does want to build products; the team just won’t commit to anything specific. The most they’ve said is that they will be building some kind of AI workplace tool, replacing products like Slack, Jira and Google Docs but also redefining how these other tools work at a fundamental level. Workplace software for a post-software workplace!\nIt’s my job to know what this stuff means, and I’m still pretty confused about that last part. But it is just specific enough that I think we can put them at Level 3.\nThinking Machines Lab\nThis is a very hard one to rate! Generally, if you have a former CTO and project lead for ChatGPT raising a $2 billion seed round, you have to assume there is a pretty specific roadmap. Mira Murati does not strike me as someone who jumps in without a plan, so coming into 2026, I would have felt good putting TML at Level 4.\nBut then the last two weeks happened. The departure of CTO and co-founder Barret Zoph has gotten most of the headlines, due in part to the special circumstances involved. But at least five other employees left with Zoph, many citing concerns about the direction of the company. Just one year in, nearly half the executives on TML’s founding team are no longer working there. One way to read events is that they thought they had a solid plan to become a world-class AI lab, only to find the plan wasn’t as solid as they thought. Or in terms of the scale, they wanted a Level 4 lab but realized they were at Level 2 or 3.\nThere still isn’t quite enough evidence to justify a downgrade, but it’s getting close.\nWorld Labs\nFei-Fei Li is one of the most respected names in AI research, best known for establishing the ImageNet challenge that kickstarted contemporary deep learning techniques. She currently holds a Sequoia-endowed chair at Stanford, where she co-directs two different AI labs. I won’t bore you by going through all the different honors and academy positions, but it’s enough to say that if she wanted, she could spend the rest of her life just receiving awards and being told how great she is. Her book is pretty good too!\nSo in 2024, when Li announced she had raised $230 million for a spatial AI company called World Labs, you might think we were operating at Level 2 or lower.\nBut that was over a year ago, which is a long time in the AI world. Since then, World Labs has shipped both a full world-generating model and a commercialized product built on top of it. Over the same period, we’ve seen real signs of demand for world-modeling from both video game and special effects industries — and none of the major labs have built anything that can compete. The result looks an awful lot like a Level 4 company, perhaps soon to graduate to Level 5.\nSafe Superintelligence (SSI)\nFounded by former OpenAI chief scientist Ilya Sutskever, Safe Superintelligence (or SSI) seems like a classic example of a Level 1 startup. Sutskever has gone to great lengths to keep SSI insulated from commercial pressures, to the point of turning down an attempted acquisition from Meta. There are no product cycles and, aside from the still-baking superintelligent foundation model, there doesn’t seem to be any product at all. With this pitch, he raised $3 billion! Sutskever has always been more interested in the science of AI than the business, and every indication is that this is a genuinely scientific project at heart.\nThat said, the AI world moves fast — and it would be foolish to count SSI out of the commercial realm entirely. On his recent Dwarkesh appearance, Sutskever gave two reasons why SSI might pivot, either “if timelines turned out to be long, which they might” or because “there is a lot of value in the best and most powerful AI being out there impacting the world.” In other words, if the research either goes very well or very badly, we might see SSI jump up a few levels in a hurry.",
  "metadata": {
    "site": "TechCrunch"
  },
  "status": "ingested",
  "blog_content": null,
  "social_content": null,
  "google_doc_id": null,
  "error_message": null,
  "created_at": "2026-01-24T09:07:33.176508",
  "updated_at": "2026-01-24T09:07:33.176508"
}